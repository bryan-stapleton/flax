import twint, re
from database.db import db, Tweet, TweetSchema
from nltk.tokenize import sent_tokenize
from nltk.sentiment.vader import SentimentIntensityAnalyzer


# Instantiates a twint config object in prep for a search. Previous search results are manually cleared to avoid artifacts.
# Looks for 'username' and 'search_term' kwargs which are passed from the front end application via get request.
# Various config options are included below, required are tagged with required. See Twint documentation for more config options that can be added here.


def TwintConfig(**kwargs):
    c = twint.Config()
    twint.output.clean_lists()
    old = twint.output.tweets_list
    if old:
        old.clear()
    if kwargs["username"]:
        c.Username = kwargs["username"]
    if kwargs["search_term"]:
        c.Search = kwargs["search_term"]
    c.Store_object = True  # creates store data object; required.
    c.Hide_output = True  # prevents spamming the terminal; required.
    c.Limit = 102  # recommended to stay at 102 for aesthetics or below 1000 for speed; required.
    # c.Profile_full = True                        # includes shadowbanned accounts & tweets. warning: slow; optional.
    return c


# Performs the search with a config object generated by TwintConfig that is passed in as the config_obj param.
# Including **kwargs for a reminder to myself for future dev purposes, currently unused -- input kwargs in the TwintConfig function instead.
# Utilizes several helper functions to clean tweet results before returning them to the front end application in json format.


def PerformSearch(config_obj, **kwargs):
    c = config_obj
    twint.run.Search(c)
    dl = int(c.Limit * 3 / 2)
    t = twint.output.tweets_list[:dl]
    retry_counter = 0
    retry_limit = 5
    while (len(t) < dl) and (retry_counter < retry_limit):
        twint.run.Search(c)
        t = twint.output.tweets_list[:dl]
        retry_counter += 1
        print(
            f"results: {len(t)} of expected {c.Limit};  attempt {retry_counter} of {retry_limit}"
        )  # for debug
        tt = RemoveDuplicates(t)[: c.Limit]
    print(f"finished: total results less duplicates: {len(tt)}")
    AnalyzeTweetSentiment(tt)
    j = SerializeTweets(tt)
    return j


# Helper function to remove duplicates from tweet results generated by PerformSearch. Takes in a list object as tweets param.
# Creates an empty set before looping through tweets and adding tweet ids that are not duplicates to the set. Returns a list object containing only non duplicates.
# This solves the mysterious problem of occasional duplicate results and does so before they get to the front end app.


def RemoveDuplicates(tweets):
    seen = set()
    cleaned = []
    for tweet in tweets:
        if tweet.id not in seen:
            seen.add(tweet.id)
            cleaned.append(tweet)
    return cleaned


# Lazy work around for serialization issue with tweet data. Gets around Flask Jsonify bug.
# TODO: Revisit this as jsonify seems to be working now, not sure what changed.


def SerializeTweets(tweets):
    jt = []
    for tweet in tweets:
        jt.append(vars(tweet))
    return jt


# Uses NLTK's VADER to analyze tweet sentiment. Returns 4 scores: pos, neg, neutral, and compound.
# The compound score is used by TweetCard.vue to generate a small badge that is green if positive (between 0 and 1), red if negative (between 0 and -1) or grey if neutral (exactly 0).
# It is recommended to keep using the compound score, but all 4 scores are shipped in case the user wishes to use the other scores for some other purpose.


def AnalyzeTweetSentiment(tweets):
    sia = SentimentIntensityAnalyzer()
    for tweet in tweets:
        scores = sia.polarity_scores(str(sent_tokenize(RemoveURL(tweet.tweet))))
        tweet.__setattr__("scores", scores)


# TODO: description
def AddToDatabase(tweets):
    _counter = 0
    for tweet in tweets:
        exists = Tweet.query.filter_by(id=tweet["id"]).first()
        if exists:
            print(f"tweet {tweet['id']} already exists in database")
            continue
        else:
            tweet_to_add = Tweet(
                id=tweet["id"],
                datetime=tweet["datetime"],
                lang=tweet["lang"],
                user_id=tweet["user_id"],
                username=tweet["username"],
                tweet=tweet["tweet"],
                score=tweet["scores"]["compound"],
            )
            try:
                db.session.add(tweet_to_add)
                print(f"tweet {tweet['id']} added to database")
                _counter += 1
            except Exception as e:
                print(e)
                continue
    db.session.commit()
    print(f"{_counter} tweets committed to db")


# TODO: Build out query function(s). Should take in query param.
def QueryTweets():
    schema = TweetSchema()
    tw = []
    # example queries
    for n in db.session.query(Tweet).all():
        # for n in db.session.query(Tweet).filter_by(username='barackobama').order_by(Tweet.datetime):
        # for n in db.session.query(Tweet).filter(Tweet.score >= 0.5):
        # for n in db.session.query(Tweet).filter_by(lang='en'):
        t = schema.dump(n)
        tw.append(t)
    return tw


# TODO: Find alternative regex expressions and test. This one seems to cause AnaylzeTweetSentiment to return some false 0s. Still notably fewer false 0s than with no regex filter.


def RemoveURL(text):
    # dont change this nonsense
    return re.sub(
        r"""(?i)\b((?:https?://|www\d{0,3}[.]|[a-z0-9.\-]+[.][a-z]{2,4}/)(?:[^\s()<>]+|\(([^\s()<>]+|(\([^\s()<>]+\)))*\))+(?:\(([^\s()<>]+|(\([^\s()<>]+\)))*\)|[^\s`!()\[\]{};:'".,<>?«»“”‘’]))""",
        " ",
        text,
    )
